# This is a failed version, I tried different ways to crawl but failed
import requests
import pandas as pd
import time

headers = {
    "User-Agent": "Mozilla/5.0",
    "Cookie": "xq_a_token=035aa2c10a884832d1c0c78cf028b04614d91de2; u=2000950018",
    "Referer": "https://xueqiu.com"
}

# ç¬¬ä¸€æ­¥ï¼šè·å–æ²ªæ·±300æˆåˆ†è‚¡åˆ—è¡¨
def get_hs300_stocks():
    url = "https://stock.xueqiu.com/v5/stock/screener/quote/list.json"
    params = {
        "page": 1,
        "size": 300,
        "order": "desc",
        "order_by": "percent",
        "market": "CN",
        "type": "sh_sz",
        "ind_code": "SH000300"  # æ²ªæ·±300æŒ‡æ•°ä»£ç 
    }
    resp = requests.get(url, headers=headers, params=params)
    data = resp.json()
    stock_list = data["data"]["list"]
    return [(stock["symbol"], stock["name"]) for stock in stock_list]

# ç¬¬äºŒæ­¥ï¼šæŠ“å–æ¯åªè‚¡ç¥¨çš„çƒ­é—¨è®¨è®º
def get_hot_discussions(symbol, name, pages=2):
    results = []
    for page in range(1, pages + 1):
        url = "https://xueqiu.com/query/v1/symbol/search/status"
        params = {
            "symbol": symbol,
            "source": "all",
            "sort": "all",
            "count": 10,
            "page": page
        }

        try:
            resp = requests.get(url, headers=headers, params=params)
            data = resp.json()
        except Exception as e:
            print(f"âŒ æŠ“å– {symbol} ç¬¬ {page} é¡µå¤±è´¥ï¼š{e}")
            continue

        for post in data.get("list", []):
            results.append({
                "è‚¡ç¥¨ä»£ç ": symbol,
                "è‚¡ç¥¨åç§°": name,
                "ä½œè€…": post.get("user", {}).get("screen_name", ""),
                "æ—¶é—´": post.get("created_at", "")[:10],
                "å†…å®¹": post.get("text", "").replace("\n", " ").strip(),
                "ç‚¹èµæ•°": post.get("like_count", 0),
                "è¯„è®ºæ•°": post.get("reply_count", 0),
                "é“¾æ¥": f"https://xueqiu.com/{post.get('user', {}).get('id')}/{post.get('id')}"
            })
        time.sleep(1.2)  # é›ªçƒæœ‰é¢‘ç‡é™åˆ¶
    return results

# ç¬¬ä¸‰æ­¥ï¼šä¸»æµç¨‹
all_data = []
stocks = get_hs300_stocks()
print(f" å…±è·å–æ²ªæ·±300æˆåˆ†è‚¡ {len(stocks)} æ”¯")

for symbol, name in stocks:
    print(f"ğŸ“Œ æ­£åœ¨æŠ“å–ï¼š{symbol} - {name}")
    data = get_hot_discussions(symbol, name, pages=2)
    all_data.extend(data)

# ç¬¬å››æ­¥ï¼šä¿å­˜ Excel
df = pd.DataFrame(all_data)
df.to_excel("æ²ªæ·±300_å…¨éƒ¨æˆåˆ†è‚¡_é›ªçƒçƒ­é—¨è®¨è®º.xlsx", index=False)

"""
import time
import pandas as pd
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

STOCK_CODE = "SH600036"  # æ‹›å•†é“¶è¡Œ
PAGES_TO_CRAWL = 3       # æƒ³æŠ“å‡ é¡µçƒ­é—¨è®¨è®º
OUTPUT_FILE = f"é›ªçƒ_{STOCK_CODE}_çƒ­é—¨è®¨è®º.xlsx"
# =============================

url = f"https://xueqiu.com/S/{STOCK_CODE}"

# å¯åŠ¨æµè§ˆå™¨ï¼ˆä¼ªè£…é˜²æ£€æµ‹ï¼‰
options = uc.ChromeOptions()
options.add_argument("--start-maximized")
driver = uc.Chrome(options=options)

# æ‰“å¼€ç›®æ ‡é¡µé¢
driver.get(url)
time.sleep(5)

# æç¤ºç”¨æˆ·æ‰‹åŠ¨ç™»å½•
input("ğŸ” è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰ï¼Œç„¶åæŒ‰ä¸‹å›è½¦ç»§ç»­...")

# ç‚¹å‡»â€œè®¨è®ºâ€æ ‡ç­¾é¡µ
try:
    discuss_tab = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.XPATH, '//span[text()="è®¨è®º"]'))
    )
    discuss_tab.click()
    print("âœ… å·²ç‚¹å‡»è®¨è®ºé¡µ")
except:
    print("âš ï¸ æœªèƒ½è‡ªåŠ¨ç‚¹å‡»è®¨è®ºé¡µï¼Œè¯·æ‰‹åŠ¨ç‚¹å‡»é¡µé¢ä¸Šçš„ã€è®¨è®ºã€æ ‡ç­¾ï¼Œç„¶åå†æŒ‰ä¸€æ¬¡å›è½¦ç»§ç»­")
    input()

time.sleep(3)

all_data = []

for page in range(PAGES_TO_CRAWL):
    print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page+1} é¡µ...")

    # ç­‰å¾…å¸–å­åŠ è½½å®Œæˆ
    WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.XPATH, '//article[contains(@class, "timeline__item")]'))
    )

    posts = driver.find_elements(By.XPATH, '//article[contains(@class, "timeline__item")]')

    print(f"ğŸ‘ æœ¬é¡µæŠ“å–åˆ° {len(posts)} æ¡å¸–å­")

    for post in posts:
        try:
            content = post.find_element(By.XPATH, './/div[contains(@class, "status__content")]').text
            user = post.find_element(By.XPATH, './/a[contains(@href, "/u/")]').text
            all_data.append({"ç”¨æˆ·": user, "å†…å®¹": content})
        except Exception as e:
            print("âš ï¸ è·³è¿‡æŸæ¡å¸–å­ï¼ŒåŸå› ï¼š", e)

    # ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®
    try:
        next_button = driver.find_element(By.XPATH, '//button[contains(@class,"next-btn") and contains(text(), "ä¸‹ä¸€é¡µ")]')
        next_button.click()
        time.sleep(3)
    except:
        print("ğŸš« æ²¡æœ‰æ›´å¤šé¡µäº†")
        break

# ä¿å­˜ä¸º Excel
df = pd.DataFrame(all_data)
df.to_excel(OUTPUT_FILE, index=False)
print(f"âœ… æ•°æ®å·²ä¿å­˜ä¸ºï¼š{OUTPUT_FILE}")

# å…³é—­æµè§ˆå™¨
try:
    driver.quit()
except:
    pass

"""