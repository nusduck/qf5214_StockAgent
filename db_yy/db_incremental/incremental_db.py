#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è‚¡ç¥¨æ•°æ®å¢é‡è·å–ç®¡é“ - æ”¹è¿›ç‰ˆ
1. å®ç°é€æ¡æ•°æ®ç«‹å³å­˜å‚¨ï¼Œé¿å…ä»»åŠ¡ä¸­æ–­å¯¼è‡´çš„æ•°æ®ä¸¢å¤±
2. å¢å¼ºå¢é‡åˆ¤æ–­é€»è¾‘ï¼ŒåŒæ—¶è€ƒè™‘æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç 
3. æ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­æ‰§è¡Œä»»åŠ¡
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import akshare as ak
import talib
from datetime import datetime, timedelta, date
import mysql.connector
from mysql.connector import Error
import logging
import traceback
import time
from demo_config import DB_CONFIG
from db_pool import get_connection, release_connection

# ä¿®å¤å¯¼å…¥è·¯å¾„
import sys
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

# ç¡®ä¿ database_connect ç›®å½•åœ¨è·¯å¾„ä¸­
database_connect_dir = os.path.join(parent_dir, 'database_connect')
if database_connect_dir not in sys.path:
    sys.path.insert(0, database_connect_dir)

# ç°åœ¨å¯¼å…¥ db_connect æ¨¡å—
from database_connect.db_connect import (
    get_stock_list, format_date, dataframe_to_sql, parse_amount, 
    convert_datetime_to_string, get_table_columns, TODAY_DATE, FIXED_START_DATE
)

# è®¾ç½®æ—¥å¿—è®°å½•
current_dir = os.path.dirname(os.path.abspath(__file__))
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(current_dir, "data_incremental.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def get_latest_date(connection, table_name, date_column, date_format='%Y-%m-%d'):
    """è·å–è¡¨ä¸­æœ€æ–°çš„æ—¥æœŸå€¼"""
    try:
        cursor = connection.cursor()
        query = f"SELECT MAX({date_column}) FROM {table_name}"
        cursor.execute(query)
        result = cursor.fetchone()[0]
        cursor.close()
        
        if result:
            # æ ¹æ®æ—¥æœŸæ ¼å¼è½¬æ¢
            if isinstance(result, datetime) or isinstance(result, date):
                return result.strftime(date_format)
            elif isinstance(result, str):
                return result
            else:
                return str(result)
        return None
    except Exception as e:
        logger.error(f"è·å–è¡¨ {table_name} æœ€æ–°æ—¥æœŸæ—¶å‡ºé”™: {e}")
        return None

# æ·»åŠ ä¸€ä¸ªæ–°çš„è¾…åŠ©å‡½æ•°æ¥å¤„ç†ç™¾åˆ†æ¯”è½¬æ¢
def convert_percentage_to_float(value):
    """å°†ç™¾åˆ†æ¯”å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    if value is None:
        return None
    
    # å¦‚æœå·²ç»æ˜¯æ•°å­—ç±»å‹ä¸”ä¸æ˜¯NaNï¼Œç›´æ¥è¿”å›
    if isinstance(value, (int, float)) and not pd.isna(value):
        return value
    
    # å¤„ç†å­—ç¬¦ä¸²ç±»å‹
    if isinstance(value, str):
        # ç§»é™¤ç™¾åˆ†å·å’Œç©ºç™½
        value = value.strip()
        if value.endswith('%'):
            try:
                # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶é™¤ä»¥100
                return float(value.rstrip('%')) / 100
            except ValueError:
                return None
        else:
            try:
                # å°è¯•ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°
                return float(value)
            except ValueError:
                return None
    
    return value

def get_processed_stocks(connection, table_name, date_column, stock_column='stock_code'):
    """è·å–æŒ‡å®šæ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨"""
    try:
        cursor = connection.cursor()
        
        # è·å–æœ€æ–°æ—¥æœŸ
        date_query = f"SELECT MAX({date_column}) FROM {table_name}"
        cursor.execute(date_query)
        latest_date = cursor.fetchone()[0]
        
        if not latest_date:
            cursor.close()
            return []
        
        # è·å–è¯¥æ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        if isinstance(latest_date, datetime) or isinstance(latest_date, date):
            latest_date_str = latest_date.strftime('%Y-%m-%d')
            stock_query = f"SELECT DISTINCT {stock_column} FROM {table_name} WHERE DATE({date_column}) = '{latest_date_str}'"
        else:
            stock_query = f"SELECT DISTINCT {stock_column} FROM {table_name} WHERE {date_column} = '{latest_date}'"
        
        cursor.execute(stock_query)
        stocks = [row[0] for row in cursor.fetchall()]
        cursor.close()
        
        return stocks
    except Exception as e:
        logger.error(f"è·å–è¡¨ {table_name} å·²å¤„ç†è‚¡ç¥¨æ—¶å‡ºé”™: {e}")
        return []

def check_table_empty(connection, table_name):
    """æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºç©º"""
    try:
        cursor = connection.cursor()
        query = f"SELECT COUNT(*) FROM {table_name}"
        cursor.execute(query)
        count = cursor.fetchone()[0]
        cursor.close()
        return count == 0
    except Exception as e:
        logger.error(f"æ£€æŸ¥è¡¨ {table_name} æ˜¯å¦ä¸ºç©ºæ—¶å‡ºé”™: {e}")
        return True  # å‘ç”Ÿé”™è¯¯æ—¶å½“ä½œç©ºè¡¨å¤„ç†ï¼Œæ‰§è¡Œå…¨é‡åŠ è½½

def insert_single_record(connection, table_name, data_dict):
    """æ’å…¥å•æ¡è®°å½•åˆ°æ•°æ®åº“"""
    try:
        cursor = connection.cursor()
        
        # æ„å»ºæ’å…¥è¯­å¥
        columns = ", ".join(data_dict.keys())
        placeholders = ", ".join(["%s"] * len(data_dict))
        values = list(data_dict.values())
        
        insert_query = f"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})"
        
        cursor.execute(insert_query, values)
        connection.commit()
        cursor.close()
        return True
    except Exception as e:
        logger.error(f"æ’å…¥å•æ¡è®°å½•åˆ°è¡¨ {table_name} æ—¶å‡ºé”™: {e}")
        logger.error(f"æ•°æ®: {data_dict}")
        connection.rollback()
        return False

def process_batch_records(connection, table_name, records, batch_size=300):
    """å°æ‰¹é‡å¤„ç†è®°å½•å¹¶æ’å…¥æ•°æ®åº“"""
    if not records:
        return 0
    
    success_count = 0
    total_records = len(records)
    
    # æŒ‰æ‰¹æ¬¡å¤„ç†
    for i in range(0, total_records, batch_size):
        batch = records[i:min(i+batch_size, total_records)]
        batch_num = i // batch_size + 1
        total_batches = (total_records - 1) // batch_size + 1
        
        try:
            # å°è¯•æ‰¹é‡æ’å…¥
            inserted = insert_batch(connection, table_name, batch)
            success_count += inserted
            logger.info(f"æ‰¹é‡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} åˆ°è¡¨ {table_name} æˆåŠŸ: {inserted} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} åˆ°è¡¨ {table_name} å¤±è´¥: {e}")
            logger.info(f"å°è¯•é€æ¡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches}")
            
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥
            batch_success = 0
            for record in batch:
                if insert_single_record(connection, table_name, record):
                    success_count += 1
                    batch_success += 1
            
            logger.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches} é€æ¡æ’å…¥å®Œæˆï¼ŒæˆåŠŸ: {batch_success}/{len(batch)} æ¡è®°å½•")
    
    return success_count


import math
import pandas as pd  # å¦‚æœä½ å·²åœ¨æ–‡ä»¶ä¸­å¯¼å…¥è¿‡å¯è·³è¿‡

def insert_batch(connection, table_name, records):
    """æ‰¹é‡æ’å…¥è®°å½•åˆ°æ•°æ®åº“"""
    if not records:
        return 0

    try:
        cursor = connection.cursor()

        # è·å–åˆ—å’Œå ä½ç¬¦
        columns = list(records[0].keys())
        if not columns:
            logger.error(f"è®°å½•æ²¡æœ‰åˆ—ï¼Œæ— æ³•æ’å…¥åˆ°è¡¨ {table_name}")
            return 0

        columns_str = ", ".join([f"`{col}`" for col in columns])
        placeholders = ", ".join(["%s"] * len(columns))
        insert_query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"

        # ğŸ”§ åœ¨è¿™é‡Œæ¸…æ´—æ‰€æœ‰è®°å½•ä¸­çš„ NaN æˆ– 'nan'ï¼Œè½¬æ¢ä¸º None
        cleaned_records = []
        for record in records:
            cleaned_record = {}
            for k, v in record.items():
                if isinstance(v, float) and (math.isnan(v) or pd.isna(v)):
                    cleaned_record[k] = None
                elif isinstance(v, str) and v.strip().lower() == 'nan':
                    cleaned_record[k] = None
                else:
                    cleaned_record[k] = v
            cleaned_records.append(cleaned_record)

        values = [list(r.values()) for r in cleaned_records]

        cursor.executemany(insert_query, values)
        connection.commit()
        cursor.close()

        return len(records)

    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥è®°å½•åˆ°è¡¨ {table_name} æ—¶å‡ºé”™: {e}")
        connection.rollback()
        return 0


def insert_dataframe_in_batches(connection, df, table_name, batch_size=300):
    """åˆ†æ‰¹æ’å…¥DataFrameåˆ°æ•°æ®åº“"""
    if df.empty:
        return True
    
    # è·å–è¡¨çš„åˆ—
    try:
        columns = get_table_columns(connection, table_name)
        logger.debug(f"è¡¨ {table_name} åˆ—å: {columns}")
        
        # åªä¿ç•™è¡¨ä¸­å­˜åœ¨çš„åˆ—
        df_columns = df.columns.tolist()
        valid_columns = [col for col in df_columns if col in columns]
        
        if not valid_columns:
            logger.error(f"DataFrameä¸­çš„åˆ—ä¸è¡¨ {table_name} çš„åˆ—æ²¡æœ‰åŒ¹é…ï¼Œæ— æ³•æ’å…¥")
            logger.debug(f"DataFrameåˆ—: {df_columns}")
            logger.debug(f"è¡¨åˆ—: {columns}")
            return False
            
        df_filtered = df[valid_columns].copy()
        
        # æ‰“å°åˆ—ä¿¡æ¯
        missing_columns = [col for col in df_columns if col not in columns]
        if missing_columns:
            logger.warning(f"ä»¥ä¸‹åˆ—ä¸åœ¨è¡¨ {table_name} ä¸­ï¼Œå°†è¢«å¿½ç•¥: {missing_columns}")
    except Exception as e:
        logger.error(f"è·å–è¡¨ {table_name} åˆ—åæ—¶å‡ºé”™: {e}")
        df_filtered = df.copy()
    
    # å¤„ç†DataFrameä¸­çš„NaNå€¼ï¼Œè½¬æ¢ä¸ºNone
    df_filtered = df_filtered.where(pd.notnull(df_filtered), None)
    
    # åˆ†æ‰¹æ¬¡å¤„ç†
    total_rows = len(df_filtered)
    success_count = 0
    
    for start_idx in range(0, total_rows, batch_size):
        end_idx = min(start_idx + batch_size, total_rows)
        batch_df = df_filtered.iloc[start_idx:end_idx]
        batch_records = batch_df.to_dict('records')
        
        batch_num = start_idx // batch_size + 1
        total_batches = (total_rows - 1) // batch_size + 1
        
        # å°è¯•æ‰¹é‡æ’å…¥
        try:
            inserted = insert_batch(connection, table_name, batch_records)
            success_count += inserted
            logger.info(f"æ‰¹é‡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} åˆ°è¡¨ {table_name} æˆåŠŸ: {inserted} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches} åˆ°è¡¨ {table_name} å¤±è´¥: {e}")
            logger.info(f"å°è¯•é€æ¡æ’å…¥æ‰¹æ¬¡ {batch_num}/{total_batches}")
            
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€æ¡æ’å…¥
            batch_success = 0
            for record in batch_records:
                try:
                    if insert_single_record(connection, table_name, record):
                        success_count += 1
                        batch_success += 1
                except Exception as rec_e:
                    logger.error(f"é€æ¡æ’å…¥è®°å½•å¤±è´¥: {rec_e}")
            
            logger.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches} é€æ¡æ’å…¥å®Œæˆï¼ŒæˆåŠŸ: {batch_success}/{len(batch_records)} æ¡è®°å½•")
    
    logger.info(f"è¡¨ {table_name} æ€»å…±æˆåŠŸæ’å…¥ {success_count}/{total_rows} æ¡è®°å½•")
    return success_count > 0

def download_company_info_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½å…¬å¸ä¿¡æ¯å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½å…¬å¸ä¿¡æ¯å¢é‡...")
    
    try:
        # è·å–å·²å­˜åœ¨çš„è‚¡ç¥¨ä»£ç 
        cursor = connection.cursor()
        cursor.execute("SELECT DISTINCT stock_code FROM company_info")
        existing_stocks = [row[0] for row in cursor.fetchall()]
        cursor.close()
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # æ‰¾å‡ºæœªå¤„ç†çš„è‚¡ç¥¨
        unprocessed_symbols = [symbol for symbol in symbols if symbol not in existing_stocks]
        
        if not unprocessed_symbols:
            logger.info("æ‰€æœ‰è‚¡ç¥¨å…¬å¸ä¿¡æ¯å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°...")
            # è¿™é‡Œå¯ä»¥å¢åŠ æ›´æ–°é€»è¾‘ï¼Œä¾‹å¦‚æ£€æŸ¥æœ€è¿‘ä¸€æ¬¡æ›´æ–°æ—¶é—´
            
        else:
            logger.info(f"å‘ç° {len(unprocessed_symbols)} åªè‚¡ç¥¨éœ€è¦è·å–å…¬å¸ä¿¡æ¯")
            
            # è·å–æœªå¤„ç†è‚¡ç¥¨çš„ä¿¡æ¯
            processed_count = 0
            total = len(unprocessed_symbols)
            
            for i, symbol in enumerate(unprocessed_symbols):
                try:
                    logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} çš„å…¬å¸ä¿¡æ¯")
                    
                    # ä½¿ç”¨akshareè·å–å…¬å¸ä¿¡æ¯
                    stock_info = ak.stock_individual_info_em(symbol=symbol)
                    
                    if not stock_info.empty:
                        # è½¬æ¢DataFrameä¸ºå­—å…¸æ ¼å¼
                        info_dict = {}
                        for _, row in stock_info.iterrows():
                            key = row['item']
                            value = row['value']
                            
                            # æ˜ å°„å­—æ®µåç§° - ç¡®ä¿ä¸db_connect.pyä¸­å®Œå…¨ä¸€è‡´
                            field_mapping = {
                                'è‚¡ç¥¨ä»£ç ': 'stock_code',
                                'è‚¡ç¥¨ç®€ç§°': 'stock_name',
                                'æ€»å¸‚å€¼': 'total_market_cap_100M',
                                'æµé€šå¸‚å€¼': 'float_market_cap_100M',
                                'æ€»è‚¡æœ¬': 'total_shares',
                                'æµé€šè‚¡': 'float_shares',
                                'è¡Œä¸š': 'industry',
                                'ä¸Šå¸‚æ—¶é—´': 'ipo_date'
                            }
                            
                            if key in field_mapping:
                                info_dict[field_mapping[key]] = value
                        
                        # æ·»åŠ è‚¡ç¥¨ä»£ç ï¼Œç¡®ä¿å­˜åœ¨
                        info_dict['stock_code'] = symbol
                        
                        # å°†æ€»å¸‚å€¼å’Œæµé€šå¸‚å€¼è½¬æ¢ä¸ºäº¿å…ƒå•ä½
                        if 'total_market_cap_100M' in info_dict and info_dict['total_market_cap_100M'] is not None:
                            try:
                                value = parse_amount(info_dict['total_market_cap_100M'])
                                if value is not None:
                                    info_dict['total_market_cap_100M'] = value / 1e8  # è½¬æ¢ä¸ºäº¿å…ƒ
                            except:
                                pass
                        
                        if 'float_market_cap_100M' in info_dict and info_dict['float_market_cap_100M'] is not None:
                            try:
                                value = parse_amount(info_dict['float_market_cap_100M'])
                                if value is not None:
                                    info_dict['float_market_cap_100M'] = value / 1e8  # è½¬æ¢ä¸ºäº¿å…ƒ
                            except:
                                pass
                        
                        # æ·»åŠ ETLå­—æ®µ
                        info_dict['snap_date'] = datetime.now().strftime('%Y%m%d')
                        info_dict['etl_date'] = datetime.now().strftime('%Y%m%d')
                        info_dict['biz_date'] = int(datetime.now().strftime('%Y%m%d'))
                        
                        # æ’å…¥æ•°æ®åº“
                        if insert_single_record(connection, 'company_info', info_dict):
                            processed_count += 1
                            logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„å…¬å¸ä¿¡æ¯")
                        else:
                            logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„å…¬å¸ä¿¡æ¯å¤±è´¥")
                    
                    else:
                        logger.warning(f"è·å–è‚¡ç¥¨ {symbol} çš„å…¬å¸ä¿¡æ¯ä¸ºç©º")
                
                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„å…¬å¸ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                
                # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
                if (i + 1) % 10 == 0:
                    time.sleep(1)
            
            logger.info(f"å…¬å¸ä¿¡æ¯å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{total} åªè‚¡ç¥¨")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½å…¬å¸ä¿¡æ¯å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("å…¬å¸ä¿¡æ¯å¢é‡ä¸‹è½½å®Œæˆ")

def download_finance_info_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½è´¢åŠ¡ä¿¡æ¯å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½è´¢åŠ¡ä¿¡æ¯å¢é‡...")
    
    try:
        # è·å–æœ€æ–°çš„æŠ¥å‘ŠæœŸæ—¥æœŸ
        latest_report_date = get_latest_date(connection, 'finance_info', 'report_date')
        
        # è·å–è¯¥æŠ¥å‘ŠæœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        processed_stocks = []
        if latest_report_date:
            processed_stocks = get_processed_stocks(connection, 'finance_info', 'report_date')
            logger.info(f"æœ€æ–°æŠ¥å‘ŠæœŸæ—¥æœŸ: {latest_report_date}, å·²å¤„ç† {len(processed_stocks)} åªè‚¡ç¥¨")
        else:
            logger.info("è´¢åŠ¡ä¿¡æ¯è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æŠ¥å‘ŠæœŸ")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # ç¡®å®šæœªå¤„ç†çš„è‚¡ç¥¨
        unprocessed_symbols = [symbol for symbol in symbols if symbol not in processed_stocks]
        
        if not unprocessed_symbols and latest_report_date:
            logger.info("å½“å‰æŠ¥å‘ŠæœŸæ‰€æœ‰è‚¡ç¥¨çš„è´¢åŠ¡ä¿¡æ¯å·²è·å–å®Œæ¯•")
            
            # å³ä½¿å½“å‰æŠ¥å‘ŠæœŸçš„æ‰€æœ‰è‚¡ç¥¨å·²å¤„ç†ï¼Œä¹Ÿå¯èƒ½æœ‰æ›´æ–°çš„æŠ¥å‘ŠæœŸéœ€è¦è·å–
            # è·å–æ›´æ–°åçš„è´¢åŠ¡æŠ¥è¡¨ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰æ–°æŠ¥å‘ŠæœŸï¼‰
            test_symbol = symbols[0]  # ä½¿ç”¨ç¬¬ä¸€åªè‚¡ç¥¨æµ‹è¯•
            try:
                test_df = ak.stock_financial_abstract_ths(symbol=test_symbol)
                if not test_df.empty and 'æŠ¥å‘ŠæœŸ' in test_df.columns:
                    test_df['æŠ¥å‘ŠæœŸ'] = pd.to_datetime(test_df['æŠ¥å‘ŠæœŸ'])
                    latest_report_date_dt = pd.to_datetime(latest_report_date)
                    newer_reports = test_df[test_df['æŠ¥å‘ŠæœŸ'] > latest_report_date_dt]
                    
                    if not newer_reports.empty:
                        logger.info(f"å‘ç°æ–°çš„æŠ¥å‘ŠæœŸæ•°æ®ï¼Œå°†ä¸ºæ‰€æœ‰è‚¡ç¥¨è·å–æ›´æ–°")
                        unprocessed_symbols = symbols  # æ‰€æœ‰è‚¡ç¥¨éƒ½éœ€è¦æ£€æŸ¥æ–°æŠ¥å‘ŠæœŸ
                    else:
                        logger.info(f"æœªå‘ç°æ¯” {latest_report_date} æ›´æ–°çš„æŠ¥å‘ŠæœŸï¼Œæ— éœ€æ›´æ–°")
                        return
                else:
                    logger.info(f"æµ‹è¯•è·å–è´¢åŠ¡æ•°æ®å¤±è´¥ï¼Œæ— æ³•ç¡®å®šæ˜¯å¦æœ‰æ–°æŠ¥å‘ŠæœŸ")
                    return
            except Exception as e:
                logger.error(f"æµ‹è¯•è·å–è´¢åŠ¡æ•°æ®æ—¶å‡ºé”™: {e}")
                return
        else:
            logger.info(f"å‘ç° {len(unprocessed_symbols)} åªè‚¡ç¥¨éœ€è¦è·å–è´¢åŠ¡ä¿¡æ¯")
        
        # å¤„ç†æœªå¤„ç†çš„è‚¡ç¥¨
        total = len(unprocessed_symbols)
        processed_count = 0
        
        # è®¾ç½®å›ºå®šèµ·å§‹æ—¥æœŸ - ç¡®ä¿ä¸€å®šä¼šå¤„ç†2024-09-24ä¹‹åçš„æ‰€æœ‰æ•°æ®
        fixed_start_date_dt = pd.to_datetime('20240924', format='%Y%m%d')
        
        for i, symbol in enumerate(unprocessed_symbols):
            try:
                logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡ä¿¡æ¯")
                
                # è·å–è´¢åŠ¡ä¿¡æ¯
                finance_df = ak.stock_financial_abstract_ths(symbol=symbol)
                
                if not finance_df.empty:
                    # åˆ›å»ºä¸€ä¸ªæ ‡å‡†çš„DataFrameæ¥ä¿å­˜ç»“æœ
                    result_rows = []
                    
                    # åœ¨è¿™é‡Œï¼Œç¡®ä¿è®°å½•æ•°é‡å’ŒæŠ¥å‘ŠæœŸåˆ—åŒ¹é…
                    if 'æŠ¥å‘ŠæœŸ' in finance_df.columns:
                        # å¯¹äºæ¯ä¸ªæŠ¥å‘ŠæœŸï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µçš„è®°å½•
                        for idx, row in finance_df.iterrows():
                            record = {
                                'stock_code': symbol,  # å§‹ç»ˆè®¾ç½®è‚¡ç¥¨ä»£ç 
                                'report_date': None,   # å°†åœ¨ä¸‹é¢è®¾ç½®
                            }
                            
                            # æ·»åŠ è‚¡ç¥¨åç§°
                            try:
                                stock_name = stock_list_df.loc[stock_list_df['ä»£ç '] == symbol, 'åç§°'].values[0]
                                record['stock_name'] = stock_name
                            except:
                                record['stock_name'] = ''
                            
                            # æ˜ å°„è´¢åŠ¡æ•°æ®åˆ—
                            column_mapping = {
                                'æŠ¥å‘ŠæœŸ': 'report_date',
                                'å‡€åˆ©æ¶¦': 'net_profit_100M',
                                'å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡': 'net_profit_yoy',
                                'æ‰£éå‡€åˆ©æ¶¦': 'net_profit_excl_nr_100M',
                                'æ‰£éå‡€åˆ©æ¶¦åŒæ¯”å¢é•¿ç‡': 'net_profit_excl_nr_yoy',
                                'è¥ä¸šæ€»æ”¶å…¥': 'total_revenue_100M',
                                'è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿ç‡': 'total_revenue_yoy',
                                'åŸºæœ¬æ¯è‚¡æ”¶ç›Š': 'basic_eps',
                                'æ¯è‚¡å‡€èµ„äº§': 'net_asset_ps',
                                'æ¯è‚¡èµ„æœ¬å…¬ç§¯é‡‘': 'capital_reserve_ps',
                                'æ¯è‚¡æœªåˆ†é…åˆ©æ¶¦': 'retained_earnings_ps',
                                'æ¯è‚¡ç»è¥ç°é‡‘æµ': 'op_cash_flow_ps',
                                'é”€å”®å‡€åˆ©ç‡': 'net_margin',
                                'æ¯›åˆ©ç‡': 'gross_margin',
                                'å‡€èµ„äº§æ”¶ç›Šç‡': 'roe',
                                'å‡€èµ„äº§æ”¶ç›Šç‡-æ‘Šè–„': 'roe_diluted',
                                'è¥ä¸šå‘¨æœŸ': 'op_cycle',
                                'å­˜è´§å‘¨è½¬ç‡': 'inventory_turnover_ratio',
                                'å­˜è´§å‘¨è½¬å¤©æ•°': 'inventory_turnover_days',
                                'åº”æ”¶è´¦æ¬¾å‘¨è½¬å¤©æ•°': 'ar_turnover_days',
                                'æµåŠ¨æ¯”ç‡': 'current_ratio',
                                'é€ŸåŠ¨æ¯”ç‡': 'quick_ratio',
                                'ä¿å®ˆé€ŸåŠ¨æ¯”ç‡': 'con_quick_ratio',
                                'äº§æƒæ¯”ç‡': 'debt_eq_ratio',
                                'èµ„äº§è´Ÿå€ºç‡': 'debt_asset_ratio'
                            }
                            
                            # ä»åŸå§‹æ•°æ®ä¸­å¤åˆ¶æ˜ å°„çš„å­—æ®µ
                            for old_col, new_col in column_mapping.items():
                                if old_col in finance_df.columns:
                                    value = row[old_col]
                                    
                                    # ä¸ºç™¾åˆ†æ¯”å­—æ®µè¿›è¡Œç‰¹æ®Šå¤„ç†
                                    if new_col in ['net_profit_yoy', 'net_profit_excl_nr_yoy', 'total_revenue_yoy',
                                                 'net_margin', 'gross_margin', 'roe', 'roe_diluted', 
                                                 'debt_asset_ratio', 'debt_eq_ratio']:
                                        value = convert_percentage_to_float(value)
                                    
                                    # ä¸ºé‡‘é¢å­—æ®µè¿›è¡Œç‰¹æ®Šå¤„ç†
                                    elif new_col in ['net_profit_100M', 'net_profit_excl_nr_100M', 'total_revenue_100M']:
                                        if value is not None:
                                            try:
                                                parsed = parse_amount(value)
                                                if parsed is not None:
                                                    value = parsed / 1e8  # è½¬æ¢ä¸ºäº¿å…ƒ
                                            except:
                                                value = None
                                    
                                    # ä¸ºå…¶ä»–æ•°å€¼å­—æ®µè¿›è¡Œå¤„ç†
                                    elif new_col in ['basic_eps', 'net_asset_ps', 'capital_reserve_ps', 
                                                   'retained_earnings_ps', 'op_cash_flow_ps']:
                                        if value is not None:
                                            value = parse_amount(value)
                                    
                                    record[new_col] = value
                            
                            # æ·»åŠ ETLå­—æ®µ
                            today_str = datetime.now().strftime('%Y%m%d')
                            record['snap_date'] = today_str
                            record['etl_date'] = today_str
                            record['biz_date'] = int(today_str)
                            
                            # ç¡®ä¿æ¯ä¸ªè®°å½•éƒ½æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç 
                            if record['stock_code'] is None:
                                record['stock_code'] = symbol
                            
                            result_rows.append(record)
                        
                        # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
                        new_df = pd.DataFrame(result_rows)
                        
                        # è½¬æ¢æŠ¥å‘ŠæœŸä¸ºæ—¥æœŸç±»å‹è¿›è¡Œè¿‡æ»¤
                        if 'report_date' in new_df.columns:
                            new_df['report_date'] = pd.to_datetime(new_df['report_date'])
                            
                            # ä¿®æ”¹è¿‡æ»¤é€»è¾‘ï¼šè€ƒè™‘æ—¥æœŸå’Œå·²å¤„ç†çš„è‚¡ç¥¨
                            if latest_report_date:
                                latest_date_dt = pd.to_datetime(latest_report_date)
                                
                                # 1. æ–°æŠ¥å‘ŠæœŸçš„æ•°æ®ä¿ç•™ OR
                                # 2. å½“å‰æœ€æ–°æŠ¥å‘ŠæœŸä½†è‚¡ç¥¨æœªå¤„ç†è¿‡çš„æ•°æ®ä¿ç•™ OR
                                # 3. ç¡®ä¿2024-09-24ä¹‹åçš„æ‰€æœ‰æ•°æ®éƒ½ä¿ç•™
                                mask = (
                                    (new_df['report_date'] > latest_date_dt) |  # æ–°çš„æŠ¥å‘ŠæœŸ
                                    ((new_df['report_date'] == latest_date_dt) & ~(new_df['stock_code'].isin(processed_stocks))) |  # ç›¸åŒæŠ¥å‘ŠæœŸä½†æœªå¤„ç†çš„è‚¡ç¥¨
                                    (new_df['report_date'] >= fixed_start_date_dt)  # 2024-09-24ä¹‹åçš„æ‰€æœ‰æ•°æ®
                                )
                                new_df = new_df[mask]
                            else:
                                # å¦‚æœæ²¡æœ‰æœ€æ–°æ—¥æœŸï¼Œç¡®ä¿è·å–2024-09-24ä¹‹åçš„æ•°æ®
                                new_df = new_df[new_df['report_date'] >= fixed_start_date_dt]
                            
                            # è½¬æ¢å›å­—ç¬¦ä¸²æ ¼å¼
                            new_df['report_date'] = new_df['report_date'].dt.strftime('%Y-%m-%d')
                        
                        if not new_df.empty:
                            # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨å¹¶æ’å…¥æ•°æ®åº“
                            records = new_df.to_dict('records')
                            
                            # ç¡®ä¿æ¯æ¡è®°å½•éƒ½æœ‰stock_code
                            for record in records:
                                if 'stock_code' not in record or record['stock_code'] is None:
                                    record['stock_code'] = symbol
                            
                            # ä½¿ç”¨æ‰¹å¤„ç†æ’å…¥
                            inserted = process_batch_records(connection, 'finance_info', records, batch_size=batch_size)
                            
                            if inserted > 0:
                                processed_count += 1
                                logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡ä¿¡æ¯: {inserted} æ¡")
                            else:
                                logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡ä¿¡æ¯å…¨éƒ¨å¤±è´¥")
                        else:
                            logger.info(f"è‚¡ç¥¨ {symbol} æ²¡æœ‰æ–°çš„è´¢åŠ¡ä¿¡æ¯æ•°æ®")
                    else:
                        logger.warning(f"è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡æ•°æ®ä¸­ç¼ºå°‘æŠ¥å‘ŠæœŸåˆ—")
                else:
                    logger.info(f"è·å–è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡ä¿¡æ¯ä¸ºç©º")
            
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„è´¢åŠ¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                logger.error(traceback.format_exc())
            
            # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 10 == 0:
                time.sleep(1)
        
        logger.info(f"è´¢åŠ¡ä¿¡æ¯å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{total} åªè‚¡ç¥¨")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½è´¢åŠ¡ä¿¡æ¯å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("è´¢åŠ¡ä¿¡æ¯å¢é‡ä¸‹è½½å®Œæˆ")

def download_individual_stock_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½ä¸ªè‚¡å†å²æ•°æ®å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½ä¸ªè‚¡å†å²æ•°æ®å¢é‡...")
    
    try:
        # æ˜ç¡®æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºç©º
        is_empty = check_table_empty(connection, 'individual_stock')
        
        # è·å–æœ€æ–°çš„äº¤æ˜“æ—¥æœŸ
        latest_date = get_latest_date(connection, 'individual_stock', 'Date')
        
        # è·å–è¯¥æ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        processed_stocks = []
        if (latest_date and not is_empty):
            processed_stocks = get_processed_stocks(connection, 'individual_stock', 'Date', 'Stock_Code')
            logger.info(f"æœ€æ–°äº¤æ˜“æ—¥æœŸ: {latest_date}, å·²å¤„ç† {len(processed_stocks)} åªè‚¡ç¥¨")
        else:
            logger.info("ä¸ªè‚¡å†å²æ•°æ®è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æ—¥æœŸï¼Œå°†æ‰§è¡Œå…¨é‡ä¸‹è½½")
        
        # è®¾ç½®å›ºå®šèµ·å§‹æ—¥æœŸ - ç¡®ä¿ä¸€å®šä¼šå¤„ç†2024-09-24ä¹‹åçš„æ‰€æœ‰æ•°æ®
        fixed_start_date = '20240924'
        fixed_start_date_dt = pd.to_datetime(fixed_start_date)
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # ç¡®å®šæœªå¤„ç†çš„è‚¡ç¥¨
        unprocessed_symbols = [symbol for symbol in symbols if symbol not in processed_stocks]
        
        logger.info(f"æ€»è‚¡ç¥¨æ•°: {len(symbols)}, å·²å¤„ç†: {len(processed_stocks)}, æœªå¤„ç†: {len(unprocessed_symbols)}")
        
        # å¤„ç†æ¯åªè‚¡ç¥¨
        total = len(symbols)
        processed_count = 0
        
        for i, symbol in enumerate(symbols):
            try:
                # ç¡®å®šæ­¤è‚¡ç¥¨çš„èµ·å§‹æ—¥æœŸ
                if latest_date and symbol in processed_stocks:
                    # å·²å¤„ç†è¿‡çš„è‚¡ç¥¨ï¼Œä»æœ€æ–°æ—¥æœŸåä¸€å¤©å¼€å§‹å¢é‡è·å–
                    latest_date_dt = pd.to_datetime(latest_date)
                    next_day = latest_date_dt + timedelta(days=1)
                    start_date = next_day.strftime('%Y%m%d')
                    logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} - å·²åœ¨æ•°æ®åº“ä¸­ï¼Œä» {start_date} è·å–å¢é‡æ•°æ®")
                else:
                    # æœªå¤„ç†è¿‡çš„è‚¡ç¥¨ï¼Œä»å›ºå®šèµ·å§‹æ—¥æœŸå¼€å§‹è·å–
                    start_date = fixed_start_date
                    logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} - æ–°è‚¡ç¥¨ï¼Œä» {start_date} è·å–å†å²æ•°æ®")
                
                # å¦‚æœèµ·å§‹æ—¥æœŸå·²ç»è¶…è¿‡ä»Šå¤©ï¼Œè·³è¿‡è¿™åªè‚¡ç¥¨
                if pd.to_datetime(start_date) > pd.to_datetime(TODAY_DATE):
                    logger.info(f"è‚¡ç¥¨ {symbol} çš„èµ·å§‹æ—¥æœŸ {start_date} è¶…è¿‡ä»Šå¤© {TODAY_DATE}ï¼Œè·³è¿‡")
                    continue
                
                # è·å–ä¸ªè‚¡å†å²æ•°æ®
                stock_df = ak.stock_zh_a_hist(
                    symbol=symbol, 
                    start_date=start_date,
                    end_date=TODAY_DATE,
                    adjust="qfq"
                )
                
                # å¦‚æœè·å–åˆ°æ•°æ®ï¼Œå¤„ç†å¹¶å­˜å‚¨
                if not stock_df.empty:
                    logger.info(f"è·å–åˆ°è‚¡ç¥¨ {symbol} ä» {start_date} åˆ° {TODAY_DATE} çš„ {len(stock_df)} æ¡æ•°æ®")
                    if 'æ—¥æœŸ' in stock_df.columns:
                        date_min = stock_df['æ—¥æœŸ'].min()
                        date_max = stock_df['æ—¥æœŸ'].max()
                        logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {date_min} è‡³ {date_max}")
                    
                    # é‡å‘½ååˆ— - ç²¾ç¡®åŒ¹é…æ•°æ®åº“å­—æ®µ
                    column_mapping = {
                        'æ—¥æœŸ': 'Date',
                        'å¼€ç›˜': 'Open',
                        'æ”¶ç›˜': 'Close',
                        'æœ€é«˜': 'High',
                        'æœ€ä½': 'Low',
                        'æˆäº¤é‡': 'Volume',
                        'æˆäº¤é¢': 'Amount_100M',
                        'æŒ¯å¹…': 'Amplitude',
                        'æ¶¨è·Œå¹…': 'Price_Change_percent',
                        'æ¶¨è·Œé¢': 'Price_Change',
                        'æ¢æ‰‹ç‡': 'Turnover_Rate'
                    }
                    
                    for old_col, new_col in column_mapping.items():
                        if old_col in stock_df.columns:
                            stock_df.rename(columns={old_col: new_col}, inplace=True)
                    
                    # å°†æˆäº¤é¢è½¬æ¢ä¸ºäº¿å…ƒå•ä½
                    if 'Amount_100M' in stock_df.columns:
                        stock_df['Amount_100M'] = stock_df['Amount_100M'] / 1e8
                    
                    # æ·»åŠ è‚¡ç¥¨ä»£ç 
                    stock_df['Stock_Code'] = symbol
                    
                    # æ—¥æœŸæ ¼å¼åŒ–ä¸è¿‡æ»¤
                    stock_df['Date'] = pd.to_datetime(stock_df['Date'])
                    
                    # ç¡®ä¿åªå¤„ç†2024-09-24ä¹‹åçš„æ•°æ®ï¼ˆå¯¹äºåˆæ¬¡è·å–çš„è‚¡ç¥¨ï¼‰
                    if symbol not in processed_stocks:
                        # è¿‡æ»¤å‡º2024-09-24åŠä¹‹åçš„æ•°æ®
                        stock_df = stock_df[stock_df['Date'] >= fixed_start_date_dt]
                    
                    # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    stock_df['Date'] = stock_df['Date'].dt.strftime('%Y-%m-%d')
                    
                    # æ·»åŠ ETLå­—æ®µ
                    today_str = datetime.now().strftime('%Y-%m-%d')
                    stock_df['etl_date'] = today_str
                    stock_df['biz_date'] = int(datetime.now().strftime('%Y%m%d'))
                    
                    # å¦‚æœè¿‡æ»¤åè¿˜æœ‰æ•°æ®ï¼Œæ‰¹é‡æ’å…¥æ•°æ®åº“
                    if not stock_df.empty:
                        if insert_dataframe_in_batches(connection, stock_df, 'individual_stock', batch_size=batch_size):
                            processed_count += 1
                            logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„å†å²æ•°æ®: {len(stock_df)} æ¡")
                        else:
                            logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„å†å²æ•°æ®å…¨éƒ¨æˆ–éƒ¨åˆ†å¤±è´¥")
                    else:
                        logger.info(f"è¿‡æ»¤åè‚¡ç¥¨ {symbol} æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                else:
                    logger.info(f"è‚¡ç¥¨ {symbol} åœ¨æ—¶é—´æ®µ {start_date} è‡³ {TODAY_DATE} æ²¡æœ‰æ•°æ®")
            
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„å†å²æ•°æ®æ—¶å‡ºé”™: {e}")
                logger.error(traceback.format_exc())
            
            # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 10 == 0:
                time.sleep(1)
        
        logger.info(f"ä¸ªè‚¡å†å²æ•°æ®å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{total} åªè‚¡ç¥¨")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½ä¸ªè‚¡å†å²æ•°æ®å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("ä¸ªè‚¡å†å²æ•°æ®å¢é‡ä¸‹è½½å®Œæˆ")

def download_stock_news_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½è‚¡ç¥¨æ–°é—»å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½è‚¡ç¥¨æ–°é—»å¢é‡...")
    
    try:
        # è·å–æœ€æ–°çš„å‘å¸ƒæ—¶é—´
        latest_publish_time = get_latest_date(
            connection, 'stock_news', 'publish_time', '%Y-%m-%d %H:%M:%S'
        )
        
        latest_publish_time_dt = None
        if latest_publish_time:
            latest_publish_time_dt = pd.to_datetime(latest_publish_time)
            logger.info(f"æœ€æ–°æ–°é—»å‘å¸ƒæ—¶é—´: {latest_publish_time}")
            
            # ç”±äºæ–°é—»æŒ‰æ—¶é—´æ’åºï¼Œè€Œä¸æ˜¯æŒ‰è‚¡ç¥¨ä»£ç æ’åºï¼Œ
            # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–è·å–å·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        else:
            # å¦‚æœè¡¨ä¸ºç©ºï¼Œä½¿ç”¨å›ºå®šèµ·å§‹æ—¥æœŸ
            logger.info("è‚¡ç¥¨æ–°é—»è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°å‘å¸ƒæ—¶é—´ï¼Œå°†ä»å›ºå®šæ—¥æœŸå¼€å§‹è·å–")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # éå†æ‰€æœ‰è‚¡ç¥¨è·å–æ–°é—»
        total = len(symbols)
        processed_count = 0
        total_news_count = 0
        
        for i, symbol in enumerate(symbols):
            try:
                logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} çš„æ–°é—»å¢é‡")
                
                # è·å–è‚¡ç¥¨æ–°é—»
                news_df = ak.stock_news_em(symbol=symbol)
                
                if not news_df.empty:
                    # åˆ›å»ºæ–°çš„DataFrameï¼Œåªä¿ç•™éœ€è¦çš„åˆ—
                    valid_cols = ['æ–°é—»æ ‡é¢˜', 'æ–°é—»å†…å®¹', 'å‘å¸ƒæ—¶é—´', 'æ–‡ç« æ¥æº', 'æ–°é—»é“¾æ¥']
                    new_df = pd.DataFrame()
                    
                    for col in valid_cols:
                        if col in news_df.columns:
                            new_df[col] = news_df[col]
                    
                    # æ·»åŠ è‚¡ç¥¨ä»£ç 
                    new_df['stock_symbol'] = symbol
                    
                    # é‡å‘½ååˆ—
                    column_mapping = {
                        'æ–°é—»æ ‡é¢˜': 'news_title',
                        'æ–°é—»å†…å®¹': 'news_content',
                        'å‘å¸ƒæ—¶é—´': 'publish_time',
                        'æ–‡ç« æ¥æº': 'source',
                        'æ–°é—»é“¾æ¥': 'news_link'
                    }
                    
                    for old_col, new_col in column_mapping.items():
                        if old_col in new_df.columns:
                            new_df.rename(columns={old_col: new_col}, inplace=True)
                    
                    # æ·»åŠ å¿«ç…§æ—¶é—´
                    new_df['snapshot_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è¿‡æ»¤å‡ºæ–°çš„æ–°é—»
                    if 'publish_time' in new_df.columns:
                        new_df['publish_time'] = pd.to_datetime(new_df['publish_time'])
                        
                        # å¦‚æœæœ‰æœ€æ–°å‘å¸ƒæ—¶é—´ï¼Œè¿‡æ»¤å‡ºæ›´æ–°çš„æ–°é—»
                        if latest_publish_time_dt:
                            old_len = len(new_df)
                            new_df = new_df[new_df['publish_time'] > latest_publish_time_dt]
                            logger.info(f"è¿‡æ»¤åæ–°é—»æ•°: {len(new_df)}/{old_len}")
                        else:
                            # å¦‚æœæ²¡æœ‰æœ€æ–°å‘å¸ƒæ—¶é—´ï¼Œè·å–ä»å›ºå®šæ—¥æœŸå¼€å§‹çš„æ–°é—»
                            try:
                                fixed_start_date_dt = pd.to_datetime(FIXED_START_DATE, format='%Y%m%d')
                                new_df = new_df[new_df['publish_time'] >= fixed_start_date_dt]
                            except:
                                pass
                        
                        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
                        new_df['publish_time'] = new_df['publish_time'].dt.strftime('%Y-%m-%d %H:%M:%S')
                    
                    if not new_df.empty:
                        # æ·»åŠ ETLæ—¥æœŸ
                        today_str = datetime.now().strftime('%Y-%m-%d')
                        new_df['etl_date'] = today_str
                        
                        # ä½¿ç”¨æ‰¹å¤„ç†æ’å…¥
                        records = new_df.to_dict('records')
                        inserted = process_batch_records(connection, 'stock_news', records, batch_size=batch_size)
                        
                        if inserted > 0:
                            processed_count += 1
                            total_news_count += inserted
                            logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„æ–°é—»: {inserted} æ¡")
                        else:
                            logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„æ–°é—»å…¨éƒ¨å¤±è´¥")
                    else:
                        logger.info(f"è‚¡ç¥¨ {symbol} æ²¡æœ‰æ–°çš„æ–°é—»æ•°æ®")
                else:
                    logger.info(f"è·å–è‚¡ç¥¨ {symbol} çš„æ–°é—»ä¸ºç©º")
            
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„æ–°é—»æ—¶å‡ºé”™: {e}")
                logger.error(traceback.format_exc())
            
            # æ¯å¤„ç†5ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 5 == 0:
                time.sleep(1)
        
        logger.info(f"è‚¡ç¥¨æ–°é—»å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{total} åªè‚¡ç¥¨ï¼Œæ€»è®¡ {total_news_count} æ¡æ–°é—»")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½è‚¡ç¥¨æ–°é—»å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("è‚¡ç¥¨æ–°é—»å¢é‡ä¸‹è½½å®Œæˆ")

def download_sector_data_incremental(connection, batch_size=300):
    """ä¸‹è½½è¡Œä¸šæ•°æ®å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½è¡Œä¸šæ•°æ®...")
    try:
        # è·å–è¡Œä¸šåˆ—è¡¨
        try:
            sector_list = ak.stock_board_industry_name_em()['æ¿å—åç§°'].tolist()
        except:
            # é¢„è®¾ä¸€äº›ä¸»è¦è¡Œä¸š
            sector_list = ['é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'ç”µå­', 'åŠå¯¼ä½“', 'åŒ»è¯', 'åŒ»ç–—', 'æ–°èƒ½æº', 'æ±½è½¦', 'æ¶ˆè´¹', 'æˆ¿åœ°äº§']
        
        all_sector_data = []
        total = len(sector_list)
        
        for i, sector in enumerate(sector_list):
            logger.info(f"å¤„ç† [{i+1}/{total}] è¡Œä¸š {sector} çš„æ•°æ®")
            
            try:
                # è·å–è¡Œä¸šæ•°æ®
                sector_df = ak.stock_board_industry_hist_em(
                    symbol=sector,
                    start_date=FIXED_START_DATE,
                    end_date=TODAY_DATE
                )
                
                if not sector_df.empty:
                    # æ·»åŠ è¡Œä¸šåç§°
                    sector_df['sector'] = sector
                    
                    # é‡å‘½ååˆ—
                    column_mapping = {
                        'æ—¥æœŸ': 'trade_date',
                        'å¼€ç›˜': 'open_price',
                        'æ”¶ç›˜': 'close_price',
                        'æœ€é«˜': 'high_price',
                        'æœ€ä½': 'low_price',
                        'æ¶¨è·Œå¹…': 'change_percent',
                        'æ¶¨è·Œé¢': 'change_amount',
                        'æˆäº¤é‡': 'volume',
                        'æˆäº¤é¢': 'amount_100M',
                        'æŒ¯å¹…': 'amplitude',
                        'æ¢æ‰‹ç‡': 'turnover_rate'
                    }
                    
                    for old_col, new_col in column_mapping.items():
                        if old_col in sector_df.columns:
                            sector_df.rename(columns={old_col: new_col}, inplace=True)
                    
                    # å°†æˆäº¤é¢è½¬æ¢ä¸ºäº¿å…ƒå•ä½
                    if 'amount_100M' in sector_df.columns:
                        sector_df['amount_100M'] = sector_df['amount_100M'] / 1e8
                    
                    all_sector_data.append(sector_df)
            
            except Exception as e:
                logger.error(f"è·å–è¡Œä¸š {sector} çš„æ•°æ®æ—¶å‡ºé”™: {e}")
            
            # æ¯å¤„ç†5ä¸ªè¡Œä¸šæš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 5 == 0:
                time.sleep(1)
        
        if all_sector_data:
            # åˆå¹¶æ‰€æœ‰è¡Œä¸šæ•°æ®
            combined_data = pd.concat(all_sector_data, ignore_index=True)
            
            # æ—¥æœŸæ ¼å¼åŒ– - è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
            combined_data['trade_date'] = pd.to_datetime(combined_data['trade_date']).dt.strftime('%Y-%m-%d')
            
            # æ·»åŠ ETLæ—¥æœŸ
            today_str = datetime.now().strftime('%Y-%m-%d')
            combined_data['etl_date'] = today_str
            
            # å†™å…¥æ•°æ®åº“
            dataframe_to_sql(connection, combined_data, 'sector')
            logger.info(f"è¡Œä¸šæ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(combined_data)} æ¡è®°å½•")
        else:
            logger.warning("æ²¡æœ‰è·å–åˆ°è¡Œä¸šæ•°æ®")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½è¡Œä¸šæ•°æ®æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("è¡Œä¸šæ•°æ®ä¸‹è½½å®Œæˆ")
 

def download_analyst_ratings_incremental(connection, batch_size=300):
    """ä¸‹è½½åˆ†æå¸ˆè¯„çº§å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½åˆ†æå¸ˆè¯„çº§å¢é‡...")
    
    try:
        # è·å–æœ€æ–°çš„æ·»åŠ æ—¥æœŸ
        latest_add_date = get_latest_date(connection, 'analyst', 'add_date')
        
        if latest_add_date:
            latest_add_date_dt = pd.to_datetime(latest_add_date)
            logger.info(f"æœ€æ–°åˆ†æå¸ˆè¯„çº§æ·»åŠ æ—¥æœŸ: {latest_add_date}")
        else:
            latest_add_date_dt = None
            logger.info("åˆ†æå¸ˆè¯„çº§è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æ·»åŠ æ—¥æœŸ")
        
        # å°è¯•è·å–åˆ†æå¸ˆæ’è¡Œ
        try:
            analyst_rank = ak.stock_analyst_rank_em(year=datetime.now().year)
            analyst_ids = analyst_rank['åˆ†æå¸ˆID'].dropna().unique().tolist()
            logger.info(f"è·å–åˆ° {len(analyst_ids)} ä¸ªåˆ†æå¸ˆID")
        except Exception as e:
            logger.error(f"è·å–åˆ†æå¸ˆæ’è¡Œå¤±è´¥: {e}")
            analyst_ids = []
            analyst_rank = pd.DataFrame()
        
        # å¤„ç†æ¯ä¸ªåˆ†æå¸ˆ
        total_analysts = len(analyst_ids)
        processed_analysts = 0
        total_ratings = 0
        
        if analyst_ids:
            for i, analyst_id in enumerate(analyst_ids):
                try:
                    logger.info(f"å¤„ç† [{i+1}/{total_analysts}] åˆ†æå¸ˆ {analyst_id} çš„è¯„çº§å¢é‡")
                    
                    # è·å–åˆ†æå¸ˆè¯„çº§
                    ratings_df = ak.stock_analyst_detail_em(analyst_id=analyst_id, indicator="æœ€æ–°è·Ÿè¸ªæˆåˆ†è‚¡")
                    
                    if not ratings_df.empty:
                        # æ·»åŠ åˆ†æå¸ˆID
                        ratings_df['analyst_id'] = analyst_id
                        
                        # é‡å‘½ååˆ—
                        column_mapping = {
                            'è‚¡ç¥¨ä»£ç ': 'stock_code',
                            'è‚¡ç¥¨åç§°': 'stock_name',
                            'è°ƒå…¥æ—¥æœŸ': 'add_date',
                            'æœ€æ–°è¯„çº§æ—¥æœŸ': 'last_rating_date',
                            'å½“å‰è¯„çº§åç§°': 'current_rating',
                            'æˆäº¤ä»·æ ¼(å‰å¤æƒ)': 'trade_price',
                            'æœ€æ–°ä»·æ ¼': 'latest_price',
                            'é˜¶æ®µæ¶¨è·Œå¹…': 'change_percent'
                        }
                        
                        for old_col, new_col in column_mapping.items():
                            if old_col in ratings_df.columns:
                                ratings_df.rename(columns={old_col: new_col}, inplace=True)
                        
                        # è¿‡æ»¤æ–°å¢çš„è¯„çº§
                        if 'add_date' in ratings_df.columns:
                            ratings_df['add_date'] = pd.to_datetime(ratings_df['add_date'])
                            if latest_add_date_dt:
                                ratings_df = ratings_df[ratings_df['add_date'] > latest_add_date_dt]
                        
                        if not ratings_df.empty:
                            # æ·»åŠ åˆ†æå¸ˆè¯¦ç»†ä¿¡æ¯
                            if not analyst_rank.empty:
                                analyst_info = analyst_rank[analyst_rank['åˆ†æå¸ˆID'] == analyst_id]
                                if not analyst_info.empty:
                                    ratings_df['analyst_name'] = analyst_info['åˆ†æå¸ˆåç§°'].values[0]
                                    ratings_df['analyst_unit'] = analyst_info['åˆ†æå¸ˆå•ä½'].values[0]
                                    ratings_df['industry_name'] = analyst_info['è¡Œä¸š'].values[0]
                            
                            # æ·»åŠ ETLå­—æ®µ
                            ratings_df['snap_date'] = datetime.now().date()
                            ratings_df['etl_date'] = datetime.now().date()
                            ratings_df['biz_date'] = int(datetime.now().strftime('%Y%m%d'))
                            
                            # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
                            if 'add_date' in ratings_df.columns:
                                ratings_df['add_date'] = ratings_df['add_date'].dt.strftime('%Y-%m-%d')
                            if 'last_rating_date' in ratings_df.columns:
                                ratings_df['last_rating_date'] = pd.to_datetime(ratings_df['last_rating_date']).dt.strftime('%Y-%m-%d')
                            
                            # ä½¿ç”¨æ‰¹å¤„ç†æ’å…¥æ•°æ®åº“
                            records = ratings_df.to_dict('records')
                            inserted = process_batch_records(connection, 'analyst', records, batch_size=batch_size)
                            
                            if inserted > 0:
                                processed_analysts += 1
                                total_ratings += inserted
                                logger.info(f"æˆåŠŸæ’å…¥åˆ†æå¸ˆ {analyst_id} çš„è¯„çº§: {inserted} æ¡")
                            else:
                                logger.warning(f"æ’å…¥åˆ†æå¸ˆ {analyst_id} çš„è¯„çº§å…¨éƒ¨å¤±è´¥")
                        else:
                            logger.info(f"åˆ†æå¸ˆ {analyst_id} æ²¡æœ‰æ–°çš„è¯„çº§æ•°æ®")
                    else:
                        logger.info(f"åˆ†æå¸ˆ {analyst_id} æ²¡æœ‰è¯„çº§æ•°æ®")
                
                except Exception as e:
                    logger.error(f"å¤„ç†åˆ†æå¸ˆ {analyst_id} çš„è¯„çº§æ—¶å‡ºé”™: {e}")
                
                # æ¯å¤„ç†5ä¸ªåˆ†æå¸ˆæš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
                if (i + 1) % 5 == 0:
                    time.sleep(1)
            
            logger.info(f"åˆ†æå¸ˆè¯„çº§å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_analysts}/{total_analysts} ä¸ªåˆ†æå¸ˆï¼Œæ€»è®¡ {total_ratings} æ¡è¯„çº§")
        
        # å¦‚æœåˆ†æå¸ˆIDè·å–å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•
        else:
            logger.info("å°è¯•ä½¿ç”¨å¤‡é€‰æ–¹æ³•è·å–æœ€æ–°è¯„çº§")
            
            try:
                # è·å–æœ€æ–°è¯„çº§ä¿¡æ¯
                latest_ratings = ak.stock_rank_forecast_cninfo(symbol="é¢„æµ‹è¯„çº§")
                
                if not latest_ratings.empty:
                    # é‡å‘½ååˆ—
                    column_mapping = {
                        'è‚¡ç¥¨ä»£ç ': 'stock_code',
                        'è‚¡ç¥¨ç®€ç§°': 'stock_name',
                        'ç ”ç©¶æœºæ„': 'analyst_unit',
                        'åˆ†æå¸ˆ': 'analyst_name',
                        'æœ€æ–°è¯„çº§': 'current_rating',
                        'è¯„çº§è°ƒæ•´': 'rating_change',
                        'æœ€æ–°ç›®æ ‡ä»·': 'trade_price',
                        'æœ€æ–°è¯„çº§æ—¥æœŸ': 'last_rating_date'
                    }
                    
                    for old_col, new_col in column_mapping.items():
                        if old_col in latest_ratings.columns:
                            latest_ratings.rename(columns={old_col: new_col}, inplace=True)
                    
                    # è¿‡æ»¤æ–°çš„è¯„çº§
                    if 'last_rating_date' in latest_ratings.columns:
                        latest_ratings['last_rating_date'] = pd.to_datetime(latest_ratings['last_rating_date'])
                        if latest_add_date_dt:
                            latest_ratings = latest_ratings[latest_ratings['last_rating_date'] > latest_add_date_dt]
                        latest_ratings['add_date'] = latest_ratings['last_rating_date']
                    
                    if not latest_ratings.empty:
                        # æ·»åŠ å…¶ä»–å¿…è¦å­—æ®µ
                        latest_ratings['snap_date'] = datetime.now().date()
                        latest_ratings['etl_date'] = datetime.now().date()
                        latest_ratings['biz_date'] = int(datetime.now().strftime('%Y%m%d'))
                        latest_ratings['industry_name'] = None  # è¿™ä¸ªå­—æ®µå¯èƒ½éœ€è¦å¦å¤–è·å–
                        
                        # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
                        if 'last_rating_date' in latest_ratings.columns:
                            latest_ratings['last_rating_date'] = latest_ratings['last_rating_date'].dt.strftime('%Y-%m-%d')
                        if 'add_date' in latest_ratings.columns:
                            latest_ratings['add_date'] = latest_ratings['add_date'].dt.strftime('%Y-%m-%d')
                        
                        # æ‰¹é‡æ’å…¥æ•°æ®åº“ï¼Œä½¿ç”¨ä¼ å…¥çš„batch_size
                        if insert_dataframe_in_batches(connection, latest_ratings, 'analyst', batch_size=batch_size):
                            logger.info(f"å¤‡é€‰æ–¹æ³•æˆåŠŸæ’å…¥è¯„çº§æ•°æ®: {len(latest_ratings)} æ¡")
                            total_ratings += len(latest_ratings)
                        else:
                            logger.warning(f"å¤‡é€‰æ–¹æ³•æ’å…¥è¯„çº§æ•°æ®å…¨éƒ¨æˆ–éƒ¨åˆ†å¤±è´¥")
                    else:
                        logger.info("æ²¡æœ‰æ–°çš„åˆ†æå¸ˆè¯„çº§æ•°æ®ï¼ˆå¤‡é€‰æ–¹æ³•ï¼‰")
                else:
                    logger.info("å¤‡é€‰æ–¹æ³•æœªæ‰¾åˆ°è¯„çº§æ•°æ®")
            
            except Exception as e:
                logger.error(f"ä½¿ç”¨å¤‡é€‰æ–¹æ³•è·å–åˆ†æå¸ˆè¯„çº§æ—¶å‡ºé”™: {e}")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½åˆ†æå¸ˆè¯„çº§å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("åˆ†æå¸ˆè¯„çº§å¢é‡ä¸‹è½½å®Œæˆ")

def download_stock_a_indicator_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½è‚¡ç¥¨æŒ‡æ ‡æ•°æ®å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½è‚¡ç¥¨äº¤æ˜“æŒ‡æ ‡æ•°æ®å¢é‡...")
    
    try:
        # è·å–æœ€æ–°çš„äº¤æ˜“æ—¥æœŸ
        latest_trade_date = get_latest_date(connection, 'stock_a_indicator', 'trade_date')
        
        # è·å–è¯¥æ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        processed_stocks = []
        if latest_trade_date:
            processed_stocks = get_processed_stocks(connection, 'stock_a_indicator', 'trade_date', 'stock_code')
            logger.info(f"æœ€æ–°è‚¡ç¥¨æŒ‡æ ‡äº¤æ˜“æ—¥æœŸ: {latest_trade_date}, å·²å¤„ç† {len(processed_stocks)} åªè‚¡ç¥¨")
        else:
            logger.info("è‚¡ç¥¨äº¤æ˜“æŒ‡æ ‡è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æ—¥æœŸ")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # ç¡®å®šæœªå¤„ç†çš„è‚¡ç¥¨
        unprocessed_symbols = [symbol for symbol in symbols if symbol not in processed_stocks]
        
        if not unprocessed_symbols and latest_trade_date:
            logger.info("å½“å‰äº¤æ˜“æ—¥æœŸæ‰€æœ‰è‚¡ç¥¨çš„æŒ‡æ ‡æ•°æ®å·²è·å–å®Œæ¯•")
        else:
            if latest_trade_date:
                logger.info(f"å‘ç° {len(unprocessed_symbols)} åªè‚¡ç¥¨éœ€è¦è·å–æŒ‡æ ‡æ•°æ®")
            else:
                logger.info(f"è¡¨ä¸ºç©ºï¼Œéœ€è¦è·å– {len(symbols)} åªè‚¡ç¥¨çš„æŒ‡æ ‡æ•°æ®")
                unprocessed_symbols = symbols
            
            # å¤„ç†æœªå¤„ç†çš„è‚¡ç¥¨
            total = len(unprocessed_symbols)
            processed_count = 0
            total_data_count = 0
            
            for i, symbol in enumerate(unprocessed_symbols):
                try:
                    logger.info(f"å¤„ç† [{i+1}/{total}] è‚¡ç¥¨ {symbol} çš„äº¤æ˜“æŒ‡æ ‡å¢é‡")
                    
                    # è·å–è‚¡ç¥¨æŒ‡æ ‡æ•°æ®
                    indicator_df = ak.stock_a_indicator_lg(symbol=symbol)
                    
                    if not indicator_df.empty:
                        # æ·»åŠ è‚¡ç¥¨ä»£ç å’Œåç§°
                        indicator_df['stock_code'] = symbol
                        try:
                            stock_name = stock_list_df.loc[stock_list_df['ä»£ç '] == symbol, 'åç§°'].values[0]
                            indicator_df['stock_name'] = stock_name
                        except:
                            indicator_df['stock_name'] = ''
                        
                        # æ—¥æœŸè¿‡æ»¤
                        indicator_df['trade_date'] = pd.to_datetime(indicator_df['trade_date'])
                        if latest_trade_date:
                            latest_trade_date_dt = pd.to_datetime(latest_trade_date)
                            indicator_df = indicator_df[indicator_df['trade_date'] > latest_trade_date_dt]
                        
                        if not indicator_df.empty:
                            # å°†æ€»å¸‚å€¼è½¬æ¢ä¸ºäº¿å…ƒå•ä½
                            if 'total_mv' in indicator_df.columns:
                                indicator_df['total_mv_100M'] = indicator_df['total_mv'] / 1e8
                                indicator_df.drop('total_mv', axis=1, inplace=True)
                            
                            # è®¡ç®—ä¸€äº›é¢å¤–æŒ‡æ ‡
                            if 'pe' in indicator_df.columns and indicator_df['pe'].notna().any():
                                indicator_df['earnings_yield'] = indicator_df['pe'].apply(
                                    lambda x: round(100 / x, 2) if x and x > 0 else None
                                )
                            
                            if 'pb' in indicator_df.columns and indicator_df['pb'].notna().any():
                                indicator_df['pb_inverse'] = indicator_df['pb'].apply(
                                    lambda x: round(1 / x, 2) if x and x > 0 else None
                                )
                            
                            # æ ¼é›·å„å§†æŒ‡æ•° = å¸‚ç›ˆç‡ Ã— å¸‚å‡€ç‡
                            if 'pe' in indicator_df.columns and 'pb' in indicator_df.columns:
                                indicator_df['graham_index'] = indicator_df.apply(
                                    lambda row: round(row['pe'] * row['pb'], 2) 
                                    if row['pe'] and row['pb'] and row['pe'] > 0 and row['pb'] > 0 
                                    else None,
                                    axis=1
                                )
                            
                            # æ·»åŠ ETLæ—¥æœŸ
                            indicator_df['etl_date'] = datetime.now().date()
                            
                            # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
                            indicator_df['trade_date'] = indicator_df['trade_date'].dt.strftime('%Y-%m-%d')
                            
                            # ä½¿ç”¨æ‰¹å¤„ç†æ’å…¥æ•°æ®åº“ï¼Œä½¿ç”¨ä¼ å…¥çš„batch_size
                            if insert_dataframe_in_batches(connection, indicator_df, 'stock_a_indicator', batch_size=batch_size):
                                processed_count += 1
                                total_data_count += len(indicator_df)
                                logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„äº¤æ˜“æŒ‡æ ‡: {len(indicator_df)} æ¡")
                            else:
                                logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„äº¤æ˜“æŒ‡æ ‡å…¨éƒ¨æˆ–éƒ¨åˆ†å¤±è´¥")
                        else:
                            logger.info(f"è‚¡ç¥¨ {symbol} æ²¡æœ‰æ–°çš„äº¤æ˜“æŒ‡æ ‡æ•°æ®")
                    else:
                        logger.info(f"è·å–è‚¡ç¥¨ {symbol} çš„äº¤æ˜“æŒ‡æ ‡ä¸ºç©º")
                
                except Exception as e:
                    logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„äº¤æ˜“æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                
                # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
                if (i + 1) % 10 == 0:
                    time.sleep(1)
            
            logger.info(f"è‚¡ç¥¨äº¤æ˜“æŒ‡æ ‡æ•°æ®å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count}/{total} åªè‚¡ç¥¨ï¼Œæ€»è®¡ {total_data_count} æ¡æ•°æ®")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½è‚¡ç¥¨äº¤æ˜“æŒ‡æ ‡æ•°æ®å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    logger.info("è‚¡ç¥¨äº¤æ˜“æŒ‡æ ‡æ•°æ®å¢é‡ä¸‹è½½å®Œæˆ")

def download_tech_indicators_incremental(connection, max_symbols=None, batch_size=300):
    """ä¸‹è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¢é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹ä¸‹è½½æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¢é‡...")
    
    # æŠ€æœ¯æŒ‡æ ‡1å¤„ç†
    try:
        # è·å–æœ€æ–°çš„äº¤æ˜“æ—¥æœŸ
        latest_trade_date_tech1 = get_latest_date(connection, 'tech1', 'trade_date')
        
        # è·å–è¯¥æ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
        processed_stocks_tech1 = []
        if latest_trade_date_tech1:
            processed_stocks_tech1 = get_processed_stocks(connection, 'tech1', 'trade_date', 'stock_code')
            logger.info(f"æœ€æ–°æŠ€æœ¯æŒ‡æ ‡1äº¤æ˜“æ—¥æœŸ: {latest_trade_date_tech1}, å·²å¤„ç† {len(processed_stocks_tech1)} åªè‚¡ç¥¨")
        else:
            logger.info("æŠ€æœ¯æŒ‡æ ‡1è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æ—¥æœŸ")
        
        # è®¾ç½®å›ºå®šèµ·å§‹æ—¥æœŸ - ç¡®ä¿ä¸€å®šä¼šå¤„ç†2024-09-24ä¹‹åçš„æ‰€æœ‰æ•°æ®
        fixed_start_date = '20240924'
        fixed_start_date_dt = pd.to_datetime(fixed_start_date)
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list_df = get_stock_list()
        symbols = stock_list_df['ä»£ç '].tolist()
        
        if max_symbols and len(symbols) > max_symbols:
            symbols = symbols[:max_symbols]
        
        # ç¡®å®šæœªå¤„ç†çš„è‚¡ç¥¨
        unprocessed_symbols_tech1 = [symbol for symbol in symbols if symbol not in processed_stocks_tech1]
        
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡1 - æ€»è‚¡ç¥¨æ•°: {len(symbols)}, å·²å¤„ç†: {len(processed_stocks_tech1)}, æœªå¤„ç†: {len(unprocessed_symbols_tech1)}")
        
        # å¤„ç†æ¯åªè‚¡ç¥¨
        total_tech1 = len(symbols)
        processed_count_tech1 = 0
        total_data_count_tech1 = 0
        
        for i, symbol in enumerate(symbols):
            try:
                # ç¡®å®šæ­¤è‚¡ç¥¨çš„èµ·å§‹æ—¥æœŸ
                if latest_trade_date_tech1 and symbol in processed_stocks_tech1:
                    # å·²å¤„ç†è¿‡çš„è‚¡ç¥¨ï¼Œä»æœ€æ–°æ—¥æœŸåä¸€å¤©å¼€å§‹å¢é‡è·å–
                    latest_date_dt = pd.to_datetime(latest_trade_date_tech1)
                    next_day = latest_date_dt + timedelta(days=1)
                    start_date = next_day.strftime('%Y%m%d')
                    logger.info(f"å¤„ç† [{i+1}/{total_tech1}] è‚¡ç¥¨ {symbol} - æŠ€æœ¯æŒ‡æ ‡1å·²åœ¨æ•°æ®åº“ä¸­ï¼Œä» {start_date} è·å–å¢é‡æ•°æ®")
                else:
                    # æœªå¤„ç†è¿‡çš„è‚¡ç¥¨ï¼Œä»å›ºå®šèµ·å§‹æ—¥æœŸå¼€å§‹è·å–
                    start_date = fixed_start_date
                    logger.info(f"å¤„ç† [{i+1}/{total_tech1}] è‚¡ç¥¨ {symbol} - æŠ€æœ¯æŒ‡æ ‡1æ–°è‚¡ç¥¨ï¼Œä» {start_date} è·å–å†å²æ•°æ®")
                
                # å¦‚æœèµ·å§‹æ—¥æœŸå·²ç»è¶…è¿‡ä»Šå¤©ï¼Œè·³è¿‡è¿™åªè‚¡ç¥¨
                if pd.to_datetime(start_date) > pd.to_datetime(TODAY_DATE):
                    logger.info(f"è‚¡ç¥¨ {symbol} çš„èµ·å§‹æ—¥æœŸ {start_date} è¶…è¿‡ä»Šå¤© {TODAY_DATE}ï¼Œè·³è¿‡")
                    continue
                
                # è·å–å†å²æ•°æ®å¢é‡
                data = ak.stock_zh_a_hist(
                    symbol=symbol, 
                    start_date=start_date,
                    end_date=TODAY_DATE,
                    adjust="qfq"
                )
                
                if not data.empty:
                    logger.info(f"è·å–åˆ°è‚¡ç¥¨ {symbol} ä» {start_date} åˆ° {TODAY_DATE} çš„ {len(data)} æ¡æ•°æ®")
                    if 'æ—¥æœŸ' in data.columns:
                        date_min = data['æ—¥æœŸ'].min()
                        date_max = data['æ—¥æœŸ'].max()
                        logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {date_min} è‡³ {date_max}")
                    
                    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    close_prices = data['æ”¶ç›˜'].values
                    high = data['æœ€é«˜'].values
                    low = data['æœ€ä½'].values
                    volume = data['æˆäº¤é‡'].values
                    
                    # è®¡ç®—MACD
                    macd, signal, hist = talib.MACD(
                        close_prices, 
                        fastperiod=5, 
                        slowperiod=10, 
                        signalperiod=30
                    )
                    
                    # è®¡ç®—RSI
                    rsi = talib.RSI(close_prices, timeperiod=14)
                    
                    # è®¡ç®—KDJ
                    k, d = talib.STOCH(high, low, close_prices)
                    j = 3 * k - 2 * d
                    
                    # ç»„ç»‡ä¿¡å·ä¿¡æ¯
                    macd_signal = ["é‡‘å‰" if h > 0 else "æ­»å‰" for h in hist]
                    rsi_signal = ["è¶…ä¹°" if r > 70 else "è¶…å–" if r < 30 else "ä¸­æ€§" for r in rsi]
                    kdj_signal = ["è¶…ä¹°" if j_val > 80 else "è¶…å–" if j_val < 20 else "ä¸­æ€§" for j_val in j]
                    
                    # æ„å»ºç»“æœDataFrame
                    tech1_df = pd.DataFrame({
                        "trade_date": data['æ—¥æœŸ'],
                        "stock_code": symbol,
                        "volume": volume,
                        "turnover_rate": data['æ¢æ‰‹ç‡'] if 'æ¢æ‰‹ç‡' in data.columns else None,
                        "RSI": rsi,
                        "MACD_DIF": macd,
                        "MACD_DEA": signal,
                        "MACD_HIST": hist,
                        "KDJ_K": k,
                        "KDJ_D": d,
                        "KDJ_J": j,
                        "macd_signal": macd_signal,
                        "rsi_signal": rsi_signal,
                        "kdj_signal": kdj_signal
                    })
                    
                    # æ—¥æœŸæ ¼å¼åŒ–ä¸è¿‡æ»¤
                    tech1_df['trade_date'] = pd.to_datetime(tech1_df['trade_date'])
                    
                    # ç¡®ä¿åªå¤„ç†2024-09-24ä¹‹åçš„æ•°æ®ï¼ˆå¯¹äºåˆæ¬¡è·å–çš„è‚¡ç¥¨ï¼‰
                    if symbol not in processed_stocks_tech1:
                        # è¿‡æ»¤å‡º2024-09-24åŠä¹‹åçš„æ•°æ®
                        tech1_df = tech1_df[tech1_df['trade_date'] >= fixed_start_date_dt]
                    
                    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    tech1_df['trade_date'] = tech1_df['trade_date'].dt.strftime('%Y-%m-%d')
                    
                    # å¦‚æœè¿‡æ»¤åè¿˜æœ‰æ•°æ®ï¼Œæ‰¹é‡æ’å…¥æ•°æ®åº“
                    if not tech1_df.empty:
                        # ä½¿ç”¨æ‰¹å¤„ç†æ’å…¥
                        records = tech1_df.to_dict('records')
                        inserted = process_batch_records(connection, 'tech1', records, batch_size=batch_size)
                        
                        if inserted > 0:
                            processed_count_tech1 += 1
                            total_data_count_tech1 += inserted
                            logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡1: {inserted}/{len(tech1_df)} æ¡")
                        else:
                            logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡1å…¨éƒ¨å¤±è´¥")
                    else:
                        logger.info(f"è¿‡æ»¤åè‚¡ç¥¨ {symbol} æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                else:
                    logger.info(f"è‚¡ç¥¨ {symbol} åœ¨æ—¶é—´æ®µ {start_date} è‡³ {TODAY_DATE} æ²¡æœ‰æ•°æ®")
            
            except Exception as e:
                logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡1æ—¶å‡ºé”™: {e}")
                logger.error(traceback.format_exc())
            
            # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 10 == 0:
                time.sleep(1)
        
        logger.info(f"æŠ€æœ¯æŒ‡æ ‡1å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count_tech1}/{total_tech1} åªè‚¡ç¥¨ï¼Œæ€»è®¡ {total_data_count_tech1} æ¡è®°å½•")
    
    except Exception as e:
        logger.error(f"ä¸‹è½½æŠ€æœ¯æŒ‡æ ‡1å¢é‡æ—¶å‡ºé”™: {e}")
        traceback.print_exc()
    
    # # æŠ€æœ¯æŒ‡æ ‡2å¤„ç† - ä¿®æ”¹åçš„ä»£ç 
    # try:
    #     # è·å–æœ€æ–°çš„äº¤æ˜“æ—¥æœŸ
    #     latest_date_tech2 = get_latest_date(connection, 'tech2', 'date')
        
    #     # è·å–è¯¥æ—¥æœŸå·²å¤„ç†çš„è‚¡ç¥¨ä»£ç 
    #     processed_stocks_tech2 = []
    #     if latest_date_tech2:
    #         processed_stocks_tech2 = get_processed_stocks(connection, 'tech2', 'date', 'stock_code')
    #         logger.info(f"æœ€æ–°æŠ€æœ¯æŒ‡æ ‡2äº¤æ˜“æ—¥æœŸ: {latest_date_tech2}, å·²å¤„ç† {len(processed_stocks_tech2)} åªè‚¡ç¥¨")
    #     else:
    #         logger.info("æŠ€æœ¯æŒ‡æ ‡2è¡¨ä¸ºç©ºæˆ–æ— æ³•è·å–æœ€æ–°æ—¥æœŸ")
        
    #     # è·å–è‚¡ç¥¨åˆ—è¡¨
    #     stock_list_df = get_stock_list()
    #     symbols = stock_list_df['ä»£ç '].tolist()
        
    #     if max_symbols and len(symbols) > max_symbols:
    #         symbols = symbols[:max_symbols]
        
    #     # æœªå¤„ç†çš„è‚¡ç¥¨
    #     unprocessed_symbols_tech2 = [symbol for symbol in symbols if symbol not in processed_stocks_tech2]
        
    #     # å¤„ç†æ—¥æœŸå’Œè‚¡ç¥¨åˆ¤æ–­
    #     if latest_date_tech2:
    #         latest_date_dt = pd.to_datetime(latest_date_tech2)
    #         start_date = (latest_date_dt + timedelta(days=1)).strftime('%Y%m%d')
            
    #         # å¦‚æœèµ·å§‹æ—¥æœŸæ™šäºæˆ–ç­‰äºä»Šå¤©ï¼Œä¸”æ‰€æœ‰è‚¡ç¥¨éƒ½å·²å¤„ç†
    #         if pd.to_datetime(start_date) >= pd.to_datetime(TODAY_DATE) and not unprocessed_symbols_tech2:
    #             logger.info("æŠ€æœ¯æŒ‡æ ‡2å·²æ˜¯æœ€æ–°æ•°æ®ï¼Œæ‰€æœ‰è‚¡ç¥¨éƒ½å·²å¤„ç†")
    #             tech2_need_process = False
    #         else:
    #             # å¦‚æœæ—¥æœŸæ˜¯æœ€æ–°çš„ä½†æœ‰æœªå¤„ç†çš„è‚¡ç¥¨
    #             if pd.to_datetime(start_date) >= pd.to_datetime(TODAY_DATE):
    #                 logger.info(f"æŠ€æœ¯æŒ‡æ ‡2æ—¥æœŸå·²æ˜¯æœ€æ–°ï¼Œä½†æœ‰ {len(unprocessed_symbols_tech2)} åªè‚¡ç¥¨å°šæœªå¤„ç†")
    #                 start_date = latest_date_tech2.replace('-', '')
    #             else:
    #                 logger.info(f"æŠ€æœ¯æŒ‡æ ‡2å¢é‡æ•°æ®èµ·å§‹æ—¥æœŸ: {start_date}, ç»“æŸæ—¥æœŸ: {TODAY_DATE}")
    #             tech2_need_process = True
    #     else:
    #         # å¦‚æœè¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤èµ·å§‹æ—¥æœŸ
    #         start_date = FIXED_START_DATE
    #         logger.info(f"æŠ€æœ¯æŒ‡æ ‡2è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤èµ·å§‹æ—¥æœŸ: {start_date}")
    #         unprocessed_symbols_tech2 = symbols
    #         tech2_need_process = True
        
    #     # å¤„ç†æŠ€æœ¯æŒ‡æ ‡2
    #     if tech2_need_process:
    #         total_tech2 = len(unprocessed_symbols_tech2)
    #         processed_count_tech2 = 0
    #         total_data_count_tech2 = 0
            
    #         for i, symbol in enumerate(unprocessed_symbols_tech2):
    #             try:
    #                 logger.info(f"å¤„ç† [{i+1}/{total_tech2}] è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡2å¢é‡")
                    
    #                 # è·å–å†å²æ•°æ®å¢é‡
    #                 data = ak.stock_zh_a_hist(
    #                     symbol=symbol, 
    #                     start_date=start_date, 
    #                     end_date=TODAY_DATE,
    #                     adjust="qfq"
    #                 )
                    
    #                 if not data.empty:
    #                     # è½¬æ¢åˆ—å
    #                     df = data.rename(columns={
    #                         "æ—¥æœŸ": "date",
    #                         "å¼€ç›˜": "open",
    #                         "æ”¶ç›˜": "close",
    #                         "æœ€é«˜": "high",
    #                         "æœ€ä½": "low",
    #                         "æˆäº¤é‡": "volume"
    #                     })
                        
    #                     # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
    #                     df['date'] = pd.to_datetime(df['date'])
                        
    #                     # æ·»åŠ è‚¡ç¥¨ä»£ç 
    #                     df['stock_code'] = symbol
                        
    #                     # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                        
    #                     # ç§»åŠ¨å¹³å‡çº¿
    #                     df['MA5'] = df['close'].rolling(window=5).mean()
    #                     df['MA20'] = df['close'].rolling(window=20).mean()
    #                     df['MA60'] = df['close'].rolling(window=60).mean()
                        
    #                     # RSI
    #                     df['RSI'] = talib.RSI(df['close'].values, timeperiod=14)
                        
    #                     # MACD
    #                     macd, signal, hist = talib.MACD(
    #                         df['close'].values,
    #                         fastperiod=12,
    #                         slowperiod=26,
    #                         signalperiod=9
    #                     )
    #                     df['MACD'] = macd
    #                     df['Signal_Line'] = signal
    #                     df['MACD_hist'] = hist
                        
    #                     # å¸ƒæ—å¸¦
    #                     middle = df['close'].rolling(window=20).mean()
    #                     std = df['close'].rolling(window=20).std()
    #                     df['BB_upper'] = middle + (std * 2)
    #                     df['BB_middle'] = middle
    #                     df['BB_lower'] = middle - (std * 2)
                        
    #                     # æˆäº¤é‡åˆ†æ
    #                     df['Volume_MA'] = df['volume'].rolling(window=20).mean()
    #                     df['Volume_Ratio'] = df['volume'] / df['Volume_MA']
                        
    #                     # ATRå’Œæ³¢åŠ¨ç‡
    #                     high = df['high'].values
    #                     low = df['low'].values
    #                     close = df['close'].shift(1).values
                        
    #                     tr1 = df['high'] - df['low']
    #                     tr2 = abs(df['high'] - df['close'].shift(1))
    #                     tr3 = abs(df['low'] - df['close'].shift(1))
                        
    #                     tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    #                     df['ATR'] = tr.rolling(window=14).mean()
    #                     df['Volatility'] = df['ATR'] / df['close'] * 100
                        
    #                     # ROC
    #                     df['ROC'] = df['close'].pct_change(periods=10) * 100
                        
    #                     # ä¿¡å·
    #                     df['MACD_signal'] = np.where(df['MACD_hist'] > 0, "é‡‘å‰", "æ­»å‰")
    #                     df['RSI_signal'] = np.where(df['RSI'] > 70, "è¶…ä¹°", 
    #                                           np.where(df['RSI'] < 30, "è¶…å–", "ä¸­æ€§"))
                        
    #                     # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…timestampç±»å‹è½¬æ¢é—®é¢˜
    #                     df['date'] = df['date'].dt.strftime('%Y-%m-%d')
                        
    #                     # ---------------ä¿®æ”¹éƒ¨åˆ†å¼€å§‹-----------------
    #                     # æ˜¾å¼æŒ‡å®šè¦æ’å…¥çš„åˆ—ï¼Œç¡®ä¿åˆ—åç¬¦åˆæ•°æ®åº“è¡¨ç»“æ„
    #                     desired_columns = ['date', 'stock_code', 'open', 'close', 'high', 'low', 'volume', 
    #                                       'MA5', 'MA20', 'MA60', 'RSI', 'MACD', 'Signal_Line', 'MACD_hist', 
    #                                       'BB_upper', 'BB_middle', 'BB_lower', 'Volume_MA', 'Volume_Ratio', 
    #                                       'ATR', 'Volatility', 'ROC', 'MACD_signal', 'RSI_signal']
                        
    #                     # ç¡®ä¿åªä¿ç•™DataFrameä¸­å­˜åœ¨çš„åˆ—
    #                     valid_columns = [col for col in desired_columns if col in df.columns]
                        
    #                     # åˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameï¼ŒåªåŒ…å«éœ€è¦çš„åˆ—
    #                     df_filtered = df[valid_columns].copy()
                        
    #                     # è®°å½•åˆ—ä¿¡æ¯ç”¨äºè°ƒè¯•
    #                     logger.debug(f"Tech2 filtered columns: {df_filtered.columns.tolist()}")
                        
    #                     # å°†DataFrameè½¬æ¢ä¸ºè®°å½•å­—å…¸
    #                     records = df_filtered.to_dict('records')
    #                     # ---------------ä¿®æ”¹éƒ¨åˆ†ç»“æŸ-----------------
                        
    #                     # æ‰¹é‡æ’å…¥æ•°æ®åº“
    #                     inserted = process_batch_records(connection, 'tech2', records, batch_size=batch_size)
                        
    #                     if inserted > 0:
    #                         processed_count_tech2 += 1
    #                         total_data_count_tech2 += inserted
    #                         logger.info(f"æˆåŠŸæ’å…¥è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡2: {inserted}/{len(df_filtered)} æ¡")
    #                     else:
    #                         logger.warning(f"æ’å…¥è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡2å…¨éƒ¨å¤±è´¥")
    #                 else:
    #                     logger.info(f"è‚¡ç¥¨ {symbol} åœ¨æ—¶é—´æ®µ {start_date} è‡³ {TODAY_DATE} æ²¡æœ‰æ•°æ®")
                
    #             except Exception as e:
    #                 logger.error(f"å¤„ç†è‚¡ç¥¨ {symbol} çš„æŠ€æœ¯æŒ‡æ ‡2æ—¶å‡ºé”™: {e}")
    #                 logger.error(traceback.format_exc())
                
    #             # æ¯å¤„ç†10ä¸ªè‚¡ç¥¨æš‚åœ1ç§’ï¼Œé¿å…APIé™åˆ¶
    #             if (i + 1) % 10 == 0:
    #                 time.sleep(1)
            
    #         logger.info(f"æŠ€æœ¯æŒ‡æ ‡2å¢é‡ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_count_tech2}/{total_tech2} åªè‚¡ç¥¨ï¼Œæ€»è®¡ {total_data_count_tech2} æ¡è®°å½•")
    
    # except Exception as e:
    #     logger.error(f"ä¸‹è½½æŠ€æœ¯æŒ‡æ ‡2å¢é‡æ—¶å‡ºé”™: {e}")
    #     traceback.print_exc()
    
    logger.info("æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¢é‡ä¸‹è½½å®Œæˆ")

def main():
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨æ•°æ®å¢é‡è·å–ç®¡é“')
    parser.add_argument('--data_types', type=str, help='è¦ä¸‹è½½çš„æ•°æ®ç±»å‹ï¼Œç”¨é€—å·åˆ†éš”(ä¾‹å¦‚: stock_info,finance,news,all)', 
                      default='all')
    parser.add_argument('--max_symbols', type=int, help='ä¸‹è½½çš„æœ€å¤§è‚¡ç¥¨æ•°é‡', 
                      default=99999)
    parser.add_argument('--batch_size', type=int, help='æ•°æ®æ‰¹å¤„ç†å¤§å°', 
                      default=300)
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œåªä¸‹è½½å°‘é‡æ•°æ®')
    # parser.add_argument('--force_full', action='store_true', help='å¼ºåˆ¶æ‰§è¡Œå…¨é‡ä¸‹è½½ï¼Œå¿½ç•¥å·²æœ‰æ•°æ®')
    
    args = parser.parse_args()
    
    # å¤„ç†å‚æ•°
    data_types = args.data_types.split(',') if args.data_types != 'all' else 'all'
    max_symbols = 10 if args.test else args.max_symbols
    batch_size = args.batch_size
    # force_full = args.force_full
    
    # if force_full:
    #     logger.info("å¯ç”¨å¼ºåˆ¶å…¨é‡ä¸‹è½½æ¨¡å¼ï¼Œå°†å¿½ç•¥å·²æœ‰æ•°æ®")
    #     # æç¤ºç”¨æˆ·ç¡®è®¤
    #     confirm = input("å³å°†æ‰§è¡Œå…¨é‡ä¸‹è½½ï¼Œè¿™å¯èƒ½ä¼šå ç”¨å¤§é‡æ—¶é—´å’Œèµ„æºã€‚ç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ")
    #     if confirm.lower() != 'y':
    #         logger.info("ç”¨æˆ·å–æ¶ˆäº†å…¨é‡ä¸‹è½½")
    #         return
    
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    connection = get_connection()
    if not connection:
        logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ï¼Œç¨‹åºé€€å‡º")
        return
    
    try:
        logger.info(f"å¼€å§‹å¢é‡æ•°æ®ä¸‹è½½ä»»åŠ¡")
        logger.info(f"æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡: {max_symbols}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        try:
            test_cursor = connection.cursor()
            test_cursor.execute("SELECT 1")
            result = test_cursor.fetchone()
            logger.info(f"æ•°æ®åº“è¿æ¥æµ‹è¯•: {result}")
            test_cursor.close()
        except Exception as db_error:
            logger.error(f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {db_error}")
            return
        
        # æ ¹æ®è¯·æ±‚ä¸‹è½½ä¸åŒç±»å‹çš„æ•°æ®ï¼Œä¼ é€’batch_sizeå‚æ•°
        
        if data_types == 'all' or 'company' in data_types:
            download_company_info_incremental(connection, max_symbols, batch_size)
        
        if data_types == 'all' or 'finance' in data_types:
            download_finance_info_incremental(connection, max_symbols, batch_size)
        
        if data_types == 'all' or 'individual_stock' in data_types:
            download_individual_stock_incremental(connection, max_symbols, batch_size)
        
        if data_types == 'all' or 'news' in data_types:
            download_stock_news_incremental(connection, max_symbols, batch_size)
        
        if data_types == 'all' or 'sector' in data_types:
            download_sector_data_incremental(connection, batch_size)
        
        if data_types == 'all' or 'analyst' in data_types:
            download_analyst_ratings_incremental(connection, batch_size)
        
        if data_types == 'all' or 'tech' in data_types:
            download_tech_indicators_incremental(connection, max_symbols, batch_size)
        
        if data_types == 'all' or 'indicator' in data_types:
            download_stock_a_indicator_incremental(connection, max_symbols, batch_size)
        
        logger.info("å¢é‡æ•°æ®ä¸‹è½½ä»»åŠ¡å®Œæˆ")
    
    except Exception as e:
        logger.error(f"å¢é‡æ•°æ®ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        traceback.print_exc()
    finally:
        # æ­£ç¡®åœ°å°†è¿æ¥é‡Šæ”¾å›è¿æ¥æ± 
        release_connection(connection)

if __name__ == "__main__":
    main()